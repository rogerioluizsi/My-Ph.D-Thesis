\section{Results}

This chapter emphasized the empirical evaluation of the performance of the most used feature effects techniques: ALE, SHAP, ME, and PD. The primary objective is to discern the differences in the explanations provided by these techniques in terms of the global effects of variables compared to the true variable effects inherent in the data-generating process. The introduced ABX metric serves as the benchmark for this evaluation. Lower ABX values are preferable as they indicate a closer alignment of the explainable technique's output with the true variable effects along all the variable ranges.

Two distinct types of supervised models were considered: RFs and NNs, with hyperparameters optimized through a cross-validation process under various conditions. It is crucial to acknowledge that the conditions under which models are applied can inherently influence their outputs and, consequently, the interpretations derived from model explanation techniques. Nevertheless, we applied both models and explanation techniques consistently across these conditions to ensure that our evaluation remains unbiased. Moreover, in all tested conditions, both models demonstrated robust performance, with a RMSD close to the standard deviation of the theoretical noise added to the target variable. This indicates that the models were effectively capturing underlying patterns in the data.

The results of the experiments are presented in Table \ref{tbl:exp1_v1} for $x_1$ and in Table \ref{tbl:exp1_v2} for $x_2$. Only values for \(k\) equal to 10 are presented, as there is no significant difference compared to the \\\(k\) equal to 50. Both models, NN and RF, performed well when measuring the root mean square deviation (RMSD). The RMSD values were found to be very close to the standard deviation of the artificial noise (0.1) added to the target variable. This suggests that, to a considerable extent, the models are effectively capturing the underlying relationship between the predictors \(x1, x2\) and the response variable \(y\) . 

\input{tables/exp1_exp2}

Examining Table \ref{tbl:exp1_v1}, it is evident that ALE outperforms the other techniques in all scenarios where data is dependent. In the hypothetical scenario of independent data - a condition that may diverge from real-world situations, where data typically exhibits some level of correlation - all techniques yield comparably satisfactory results for the RF model, with ME producing the best values. However, while ME most accurately captures the effects of variable 1 ($x_1$), it produces suboptimal results for variable 2 ($x_2$) (Table \ref{tbl:exp1_v2}), which has no effect on $y$ in this independent scenario. It is probable that some effect was assigned to the $x_2$ through $x_1$ even variables being independent of each other. This highlights the pitfalls of using the data joint distributions without interventions when explaining models under independent data.

In the same independent scenario, permutation-based techniques (PD and SHAP), which perform interventions (albeit at the cost of extrapolation), produce commendable results for the RF model but not for the NN model. Initially, this discrepancy may be attributed to the high flexibility of NNs \cite{Grinsztajn2022WhyData}, which can potentially yield many functions consistent with the observed data but divergent from the true data-generating process. This flexibility can lead to a mismatch between explanations derived from the NN model and the actual data-generating process. 

Upon closer examination, however, ALE and ME, which unlike PD and SHAP do not extrapolate the training data, exhibit comparably low ABX across both RF and NN models within the independent scenario. Consequently, the discrepancy from RF to NN in SHAP and PD outputs, indeed, suggests that the extrapolation can be problematic even in independent scenarios when explaining highly complex models such as NNs. NN may inadvertently yield unusual predictions outside of the training data, thus compromising their ability to capture the effect even from the independent features due to the model's complex behavior.

Conversely, step-wise algorithms, such as RF, tend to exhibit greater stability in their predictions owing to their construction from multiple decision trees, which individually handle variations in data in a more controlled manner.

The quantity of data points significantly affects the outcomes in the independent scenario. Generally, an increase in data points enhances the performance of all examined techniques within both algorithms. This trend is also observable in the dependent scenario for ALE, which slightly improves ABX as the number of data points increases. Although a similar improvement can be observed for other techniques, it is not enough to decrease the ABX to levels comparable to those achieved by ALE.

The results for ALE are even more favorable when considering the variable $x_2$ as shown in Table \ref{tbl:exp1_v2}. In the independent scenario, all techniques failed to assign no effect to $x_2$ for both NN and RF models. In other scenarios, where the data are dependent, ALE achieves better results, while the other techniques exhibit higher ABX values. 




