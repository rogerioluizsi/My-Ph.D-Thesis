\chapter{Introduction}
\label{contextualizacao}
This thesis defines and applies more suitable global eXplainable Artificial Intelligence (XAI) techniques within the educational domain to conduct Knowledge Discovery in Databases (KDD) more effectively. Bridging the fields of Machine Learning (ML) and Education, which each have their own distinct terminologies, this thesis uses the terms 'variable,' 'feature,' and 'predictor' synonymously. They denote individual and measurable attributes or characteristics of the observed phenomena within the datasets.  Furthermore, the terms "label", "target", "dependent variable" and "outcome" are often used interchangeably and refer to the variable whose values are to be estimated.  Finally, when using the term 'marginal effects' is employed here either in the sense common to econometrics, denoting the 'additional' effect, or in relation to the probability distribution of a variable. In both occasions, context is added as an attempt to prevent  misinterpretation. 

\section{Contextualization}

In modern society, data is a critical resource for guiding human decision-making processes \cite{J.Han2012DataTechniques}. More recently, with the technological advances of the twentieth cents, our capability to store, process, and analyze large volumes of data has put forward the data potential to enhance human activities. As we transition into this new era characterized by data ubiquity, emergent paradigms in data analysis have arisen to meet the challenges and opportunities presented by this voluminous and complex data landscape. 

In 2001, Breiman called for the use of an algorithm approach as a more accurate and informative alternative to the use of data to solve problems \cite{Breiman2001StatisticalAuthor}. The algorithmic modeling he refers to is ML, which, unlike traditional approaches which adjust data for a predefined model, learns empirically from data to estimate functions for making predictions on new data. According to Breiman, ML tools facilitate a move away from exclusive reliance on parametric models, adopting a more diverse set of tools. This approach could enable researchers to move beyond confirmatory research based on theory models and also allow them to derive new theories directly from data \cite{Molina2019AnnualSociology}.

In supervised ML \cite{Vapnik1999AnTheory}, models are iteratively optimized to minimize out-of-sample prediction error, a focus that diverges from disciplines more concerned with understanding the underlying data-generating processes \cite{Mullainathan2017MachineApproach}. However, \cite{Shmueli2010ToPredict, Zhao2021CausalModels} argue that a model exhibiting both strong predictive performance and consistent assumptions closely approximates the underlying natural laws governing the data. This dual focus not only highlights the significance of ML's predictive capabilities but also the critical importance of ensuring that models align with real-world phenomena. Further reinforcing this notion is the stance taken by \cite{Cao2009IntroductionMining}, who stresses the importance of aligning data mining models with complex real-world challenges. Cao advocates for the integration of domain-specific knowledge throughout the entire Knowledge Discovery in Databases (KDD) process, a strategy that promises to deliver more reliable and actionable insights.

Nonetheless, the emphasis on predictive performance in ML has prompted researchers to adopt increasingly complex models, often at the expense of interpretability. For instance, the coefficients in additive linear models or the rules derived from decision trees offer straightforward interpretability, explicitly mapping input features to model outputs\cite{molnar2019}. In contrast, opaque models like neural networks and ensemble methods, though potentially superior in prediction, do not readily reveal the mechanisms relating input features to outcomes\cite{Linardatos2021ExplainableMethods}. The complexity of these models poses significant challenges for interpretation within a KDD process, especially when seeking scientific insights and explanations for wrong decisions made, particularly before the Justice. 

Within this context, and given the widespread adoption of ML in many tasks,  the field of XAI has quickly become an important focus within the larger field of ML. XAI aims to explain the reasoning and decision-making processes of these models in a human-understandable manner \cite{Miller2019ExplanationSciences}. These explanations are valuable not only for applications aimed at deriving insights from data but also for those whose primary objective is prediction. For example, merely categorizing a patient's health status in a hospital or predicting a student's likelihood of dropping out lacks utility without understanding the contributing factors. It's more beneficial to explore the conditions leading to such predictions to facilitate appropriate interventions \cite{Razavian2015Population-levelFactors, Pellagatti2021GeneralizedDropout, Berens2019EarlyMethods}.  Furthermore, the transparency of ML models in sectors like criminal justice \cite{Wang2023InPrediction} and finance \cite{Bussmann2021ExplainableManagement, Chen2023Globally-ConsistentEvaluation} are increasingly mandated by legal and ethical considerations. 






