\subsubsection{Global SHAP explanations}
\label{chap2_shap}

The aim of SHAP is to clarify individual model predictions by quantifying the contribution of each feature via the Shapley Values (SV) \cite{Shapley1953AGames}. Initially designed for local interpretability, SHAP can be seamlessly adapted for global model explanation by aggregating individual feature contributions \cite{Lundberg2020FromTrees.}. The most used SHAP-based score to indicate feature contribution is the absolute average of SV for each feature. Owing to its robust theoretical foundation and extensive implementations as software libraries, SHAP has emerged as a predominant technique in both industrial applications and academic research \cite{Bhatt2020ExplainableDeployment}.

The SV derives from coalitional game theory, where each feature value assumes the role of a player in a cooperative game, and the model prediction represents the total value or payout of the coalition. The SV is designed to allocate this collective payout equitably among the contributing features. Specifically, the SV $\phi_v(i)$ for a player $i$ with a characteristic function $v$ is computed as follows: 

\begin{equation}
\phi_v(i) = \sum_{S \subseteq N \backslash \{i\}} \frac{|S|! \left( |N| - |S| - 1 \right)!}{|N|!} \left( v\left( S \cup \{i\} \right) - v(S) \right)
\end{equation}

Where the summation iterates over all possible coalitions $S$ that exclude the player \(i\), thereby calculating the average additional contribution of player $i$ across all these coalitions. The term $|S|$ denotes the cardinality of coalition $S$ while $|N|$ indicates the cardinality of the complete set of players $N$. 

The fraction $\frac{|S|! \left( |N| - |S| - 1 \right)!}{|N|!}$ function as the weighting factor for each coalition $S$. It quantifies the number of ways to form $S$ and then adds $i$ relative to the total number of ways to form any coalition, including $i$. 

Finally,  the term$\left( v\left( S \cup \{i\} \right) - v(S) \right)$ calculates the additional contribution of player $i$ to coalition $S$. 

When accounting for all possible coalitions, SHAP assumes feature independence and integrates over the marginal distribution, akin to PD Plots and other permutation-based explainability techniques. Consequently, this approach introduces the issue of extrapolation.

SHAP  inherits axiomatic properties from SV, namely Efficiency, Symmetry, Dummy, and Additivity.

\begin{itemize}
    \item \textbf{Efficiency}, also termed as local accuracy in the SHAP context \cite{10.5555/3295222.3295230}, stipulates that the sum of SV for all features must equate to the total predictive value generated by the coalition of all features.
  
    \item \textbf{Dummy} axiom pertains to features that do not affect the model's prediction; such features are allocated a zero SV, reflecting their lack of contribution.

    \item \textbf{Additivity} or \textbf{Linearity} property is particularly relevant in post-hoc interpretability settings. It posits that the total attribution of a feature is the summation of all SVs associated with that feature across different models or scenarios.
\end{itemize}

In addition to these inherited properties, SHAP introduces unique attributes:

\begin{itemize}
    \item \textbf{Missingness} is designed to uphold the Efficiency property during the SHAP computation, especially when data may be incomplete or missing.
  
    \item \textbf{Consistency} ensures that the attribution of a feature changes in correlation with its SV. If a feature becomes more important, its attribution should increase correspondingly, and vice versa.
\end{itemize}

Consequently, SHAP can be represented as an additive feature attribution method.

\begin{equation}
g(z')=\phi_0+\sum_{v=1}^M\phi_v'
\end{equation}

Where $g$ is the explanation model, $z'\in\{0,1\}^M$ is the number of simplified input features - a binary vector indicates the presence or absence of a given feature within the coalition $S$.



