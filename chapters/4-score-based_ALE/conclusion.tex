\section{Summary}

This chapter introduces four novel metrics - MUA, UAS, AUA,  and AAR - derived from ALE that quantify the significance of features within predictive models. Each metric elucidates a particular aspect of feature relevance with explicit definitions. The development of these metrics is primarily motivated by the limitations of current measures used in EDM, which often fail to adequately emphasize the contributions of features in datasets where dependent variables are interrelated. The experimental evaluation demonstrates the enhanced robustness of the ALE-based framework in such contexts when compared to established methodologies. Specifically, a first round of experiments illustrates the ability of the new metrics to effectively identify critical features in synthetic datasets engineered with varying generating functions and degrees of feature interdependence. These new metrics either surpass or match the performance of existing baseline measures. Specifically, the ALE-based metric generally yielded better results compared to SHAP and MDI, as they do not attribute significance to features that are irrelevant yet highly correlated with another relevant feature. Furthermore, they also outperformed PFI and cs\_PFI in identifying multiple important features that are also correlated among them. When evaluating both properties, ALE got the better results.  

We focus on RF and NN. Both models demonstrated good performance, with an RMSE close to the standard deviation of the theoretical noise added to the target variable, indicating that the models were capable of detecting underlying patterns

Further, empirical evidence from real-world datasets corroborates the findings from synthetic experiments, showcasing the ALE-based metrics' capacity to discern the main effects of variables. In a qualitative experiment utilizing an educational dataset and domain expertise to assess the expected relevance of key variables, ALE-based scores yielded better results than the baseline. The ALE scores were able to accurately identify and isolate the relevance of variables without attributing significance to potentially irrelevant features solely due to their correlation with relevant ones.

Moreover, the chapter discusses the potential pitfalls of using SHAP-based scores, particularly when computed through the computationally efficient TreeShap method for tree-based algorithms. It can lead to misleading interpretations due to its constraints to avoid extrapolation based on the tree structure of the explained model.

Finally, the potential of the proposal for feature selection and dimensional reduction tasks was demonstrated using openly real-world datasets, including one from the educational domain and two high-dimensional datasets which are commonly used in feature selection benchmarking. The experiments showcased the ALE framework as a reliable model-agnostic alternative for this purpose. The ALE-based score achieves better or comparable results when contrasted with the widely used feature importance of random forest (MDI) in various scenarios. The ALE could remove irrelevant variables while increasing or keeping the model performance even in scenarios inherently biased in favor of the baseline.
