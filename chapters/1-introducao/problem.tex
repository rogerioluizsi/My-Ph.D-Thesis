\section{Problem Statement}

In the realm of supervised learning within EDM, consider the function \( Y = f(X) \), where \( X = (x_1, x_2, \ldots, x_n) \) represents a set of structured variables, \( M \) denotes a matrix depicting their interdependencies and \(G\) a post-hoc XAI model. Assuming \( f \) is an effectively performing model and can be considered as a source of knowledge about the underlying phenomenon, insights about \( X \) can be gleaned from \(G\) by analyzing how \( f \) utilizes \( X \) to predict \( Y \), especially when considering the dependencies encapsulated in \( M \).

Moreover, in the context of global explanations, it is expected that the explanation contained in \(G\) represents the individual role of each component to which \(G\) refers. Specifically, the attributed relevance to \(x_1 = G(x_1)\) should pertain solely to the role of \(x_1\). Similarly, \(G(x_1, x_2)\) should represent the combined effect of \(x_1\) and \(x_2\) exclusively. This property will ensure an interpretation of \(G\) akin to the coefficients of linear models, where the \(\beta\) values indicate the individual impact of each feature, assuming the model's conditions are met. Nevertheless, if \(M\) is not taken into account or is not correctly handled by \(G\), biases may skew the interpretation of individual components of \(f\), mixing effects, resulting in mixed effects of features or even unrealistic feature representations.

A review of the literature on applying XAI in EDM primarily focusing on extracting knowledge from data,  presented in Section \ref{chap2}, indicates that commonly used techniques for explaining opaque models may result in unrealistic explanations of \(X\). This is largely due to a lack of constraints to address \(M\) in the computation of \(G\). Specifically, the review highlights the use of PD and SHAP as the predominant tools for informing feature effects through plots.

The PD plots, introduced in 2001 by Friedman \cite{Friedman2001GreedyMachine.}, serves as a pioneering technique for visualizing the effects of predictors. In recent years, SHAP \cite{10.5555/3295222.3295230} has gained widespread acceptance in both industrial and academic settings \cite{Bhatt2020ExplainableDeployment} for delineating local and global effects of features. However, prevalent implementations of PD and SHAP are permutation-based techniques, which overlook the matrix \(M\). This oversight can compromise the accurate interpretation of the individual relevance of components in \(f\). 

In educational data mining, one example of how this problem can arise involves variables related to socioeconomic factors. Socioeconomic status is often gauged using multiple correlated variables, such as parental education level, access to cultural resources, and availability of technological devices. These variables frequently play a crucial role in many educational contexts. Variables that may not be inherently significant to an underlying predictive function but are correlated with those significant ones could be inappropriately emphasized. Moreover, it is vitally important for educational practitioners that the components of  \(G\) reflect the isolated effect of the individual contribution of each of these significant variables or their interactions without bias due to correlations.

The Accumulated Local Effects (ALE) plots \cite{Apley2020VisualizingModels} were recently introduced as an alternative to elucidate feature effects. The primary motivation behind ALE is to address the extrapolation problem by ensuring a pseudo-orthogonality property. This property enables ALE to approximate orthogonality — where components operate independently from each other — thereby facilitating the isolation of individual explanations for each component within a predictive function. Consequently, ALE presents itself as an effective and straightforward alternative for explaining models where data exhibits significant dependence as is common in most EDM applications. However, to the best of our knowledge, it has not yet been employed. 

Moreover, it's important to note that ALE has primarily been used to illustrate feature effects through plots of partial derivatives. These plots demonstrate the varying effects of a single variable across its value range, offering detailed insights. However, this level of detail may not always be practical. For example, when analyzing multiple variables simultaneously, such complexity can reduce human intelligibility, considering that an average person is capable of processing only a limited amount of information at once \cite{Miller1956TheInformation.}. Furthermore, plots are not feasible to describe interactions between more than two features \cite{Apley2020VisualizingModels}.

In the context of KDD's systematic processes, simpler metrics like scores offer a pragmatic approach. They can initially identify key variables, which then facilitates more in-depth exploration of the roles these features play. Scores are also useful for feature selection, providing a straightforward method for determining the most influential variables. Furthermore, scores are well-suited for comparative and trend analysis, aiding in the evaluation of feature relevance across different educational systems and over time. This adaptability makes scores a valuable tool for broader analysis in educational settings, akin to the traditional coefficients in linear models.

Building upon the review presented in Chapter \ref{chap2}, it becomes evident that methods such as the Mean Decrease in Impurity (MDI) derived from tree-based models, along with the Permutation Feature Importance (PFI) and average SHAP values, are among the most prominent techniques for elucidating feature attribution through scores. The MDI assesses a feature's relevance by averaging the degree to which the feature is used as a split criterion during tree construction. Its variations PFI,  which was initially designed for random forests and further extended for other tree-based models, was also further conceptualized to be applied in a model-agnostic manner. The PFI takes the difference (or ratio) of model performance between the baseline model and the model when the feature is randomly permuted. The PFI is designed to provide a score for each feature based on how much diﬀerence replacing the feature with noise makes in predictive performance. PFI addresses some well-known limitations of MDI, such as its bias towards features with a high number of categories and continuous variables \cite{Li2019AForests}. However, PFI, as the average SHAP, is a permutation-based method that computes explanations out-of-distribution, thus raising concerns about their applicability in assessing the importance of features in dependent datasets \cite{Strobl2008ConditionalForests, Rudin2019StopInstead, Nembrini2019BiasRecommendations, Nicodemus2011LetterMeasures}. 

Beyond the challenge of managing dependent data, ranking-based metrics, which illustrate the relative importance of variables in model performance such as PFI may not effectively illustrate the direct relationship between features and the target variable. This is a crucial aspect for EDM practitioners who seek to glean insights from data. It would be more advantageous if these scores also represented the individual contributions of features to predictions, rather than solely to performance. The scores should be able to be either positive or negative, indicating the specific nature of the relationship between the features and the target variable. Adopting this approach would align more closely with the interpretation of coefficients in traditional linear models, which are commonly used in education.

Therefore, this thesis problem centers on the need for a robust method that can elucidate the isolated roles of each component in the predictive function. Given this challenge and ALE properties, the following question guides this research: Can ALE be incorporated into the range of XAI tools to be used in EDM to inform more accurate feature contribution either by plots and scores, even when data are not independent? 






