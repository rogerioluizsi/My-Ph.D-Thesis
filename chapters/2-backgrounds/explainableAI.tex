\section{XAI}

The wide use of Artificial Intelligence (AI) and ML has increasingly emphasized the importance of transparency and user comprehension of model behaviors, primarily under the terms "Explainable AI" and "Interpretable ML" (IML). Despite the significant growth of this research area in recent years \cite{Arya2019OneTechniques}, foundational works in the field can be traced back to the 1980s \cite{Fagant1980COMPUTER-BASEDVM, Bareiss1988Protos:Apprentice}. While some authors argue that XAI and IML can be conceptually distinct \cite{Watson2022ConceptualLearning}, they are more commonly used interchangeably in the broader scientific literature as both terms share the objective of enhancing the transparency of ML models \cite{Molnar2022Model-agnosticLearning}

In this thesis, the term "interpretability" will used to refer to its common dictionary meaning, while "explainability" will be specifically employed to describe the systematic extraction of knowledge about predictive models. 

\subsection{Inherent Interpretable Models}

Inherent interpretable models, often referred to as intrinsically interpretable models, are those models distinguished by their transparent and easily understandable internal mechanics. These model provide explicit explanations of the relationships between input features and output predictions, facilitating a deeper understanding of their recommendation in the decision-making processes. 

Examples of such models include linear regression, decision trees, and induction rules. In the linear regression \ref{lm}, for instance, each $\beta$ coefficient quantifies the change in a dependent variable for a one-unit change in an independent variable, assuming all other variables are held constant. In other words, the model additive parametrization allows an isolated interpretation of the effects of individual features. Many other adaptions allow a linear model to capture more complex relationships \cite{TrevorHastieRobertTibshirani2014AssessmentSelection}, such as interactions and non-linearity. Nevertheless, in models that involve transformations of this linear predictor into other discrete outcomes, such as in the logit and probit models, the $\beta$ interpretation is not straightforward and limited \cite{Mood2017LogisticHeterogeneity, long1997regression}. 

The decision trees categorize outcomes based on decision rules at each node. On the other hand, inducing rules out of tree structure do not narrow the dimensional space as it occurs in the trees. These induced rules are clear statements in the natural language of how inputs lead to outputs in several different perspectives \cite{J.Han2012DataTechniques}.

The primary advantage of using inherent interpretable models is their ease of interpretation, which is especially beneficial to high-stakes decisions \cite{Rudin2019StopInstead}. However, they are often outperformed by more complex models when it comes to predictive performance \cite{Loyola-Gonzalez2019Black-boxView}. The simplicity that makes them easy to interpret can also be a drawback, as it might lead to the oversimplification of intricate relationships in the data. This can be a significant limitation when dealing with complex systems where multiple variables interact nonlinearly.

\subsection{Post-hoc Explainable Techniques}

When a second model is used to explain the first, it is categorized as a post-hoc explainable technique. Model-agnostic explanation model is any function \(G\) that approximates the original model \(F\) \cite{10.5555/3295222.3295230}. While intrinsically interpretable models provide insights into predictions using their internal components, post-hoc techniques treat models as opaque, relying solely on their prediction function and data \cite{Molnar2022Model-agnosticLearning}. 

Post-hoc techniques are typically model-agnostic and offer the flexibility to explain various types of models, including those that are transparent, in an effort to enhance existing explanations. For instance, in a logistic regression model, a model-agnostic technique can illustrate the individual feature effects across the entire range of the feature values, whereas the coefficients only indicate the order of feature contributions. Similarly, understanding feature effects can complement the intrinsic rule-based explanations provided by a decision tree. 

\subsection{Measuring explainability}

Explainability is a domain-specific notion and has a big criticism of the lack of formalization \cite{Rudin2019StopInstead,Watson2022ConceptualLearning,Lipton2018TheInterpretability}. Explanations can take various forms, and there isn't a clear definition of what constitutes an explanation. Moreover, explanations can differ based on the type of input variables. For images, explanations are often visualized as heatmaps, whereas for text inputs, they typically involve highlighting text passages or emphasizing words \cite{Molnar2022Model-agnosticLearning}.

This thesis focuses on tabular data. For certain tasks within this domain, visual graphs by using plots may be the preferred form of explanation, while others might favor text or scores. This variation makes it challenging to find a widely accepted definition of explainability, even within this narrowed scope. Such diversity in explanations presents a challenge in defining quantifiable evidence for the field \cite{Doshi-Velez2017TowardsLearning}. 

Unlike supervised learning, where much of the literature has advanced based on clear benchmarks of model performance, the sub-field of XAI or IML still faces vagueness in definitions. This is due to the challenge of measuring the trustworthiness of model explanations, as there is no ground truth for comparison in the real world which is only known by its observable data. Determining which explanation is superior is also difficult \cite{Arya2019OneTechniques}, even for inherently interpretable models. For instance, we can't always say whether a decision tree path is more or less clear than a linear model's coefficients \cite{Molnar2022Model-agnosticLearning}.

Considering the variety of ways in which explanations can be derived, their evaluation depends on the intended purpose of use. For example, one can assess how effectively humans utilize explanations \cite{Ehsan2021OperationalizingAI,Wang2019DesigningAI}, or evaluate the explanatory function itself by measuring aspects such as size or sparsity \cite{Yang2017ScalableLists, Ustun2016SupersparseSystems, Claassen2013LearningNP-hard}. Additionally, it is possible to quantify certain aspects related to explanations, such as evaluating the extent to which explanations predict model outputs \cite{Chen2022Use-Case-GroundedEvaluation, Lakkaraju2016InterpretableSets}. For benchmarking purposes, a common practice is the use of synthetic data with a known data-generating process. This approach facilitates the comparison of actual explainability with expected explanations in various contexts.

\subsection{Who needs explanation?}

The demand for ML explanations is pertinent across various sectors with the specific needs and objectives varying by domain and stakeholder diversity. Model explainability is not merely a desirable attribute but can be a crucial aspect for reasons ranging from model debugging to scientific exploration. This section delineates the roles of key stakeholders and the significance of explanations beyond performance within their respective domains.

Model creators, typically the developers of ML models, find interpretability crucial for debugging tasks \cite{Bhatt2020ExplainableDeployment}. It is important to know how the model relies on features to make predictions in order to fix unexpected behavior. For instance, the model creator might be interested in a model that makes decisions based on meaningful features rather than sensitive features in order to enhance generalizability or fairness. Such scrutiny cannot be achieved by only observing performance.

Operators, who use a model's outputs in their tasks, also require an understanding of the decision-making rationale. For instance, merely classifying a patient in a hospital into a particular health status is not particularly helpful. It is more useful to investigate the conditions that have contributed to this \cite{Razavian2015Population-levelFactors}, and this becomes even more crucial in the event of legal matters. Additionally, in the education domain, understanding why a student might drop out is more valuable than just predicting it \cite{Pellagatti2021GeneralizedDropout,Berens2019EarlyMethods}. This is because, as in medicine, the treatment depends on the probable cause.

The people who are subject of decision-making also have to get a kind of explanation. For instance, a loan approval model may recommend the rejection of an applicant based on specific financial variables. Understanding the rationale behind a decision empowers the applicant to make informed future choices or contest an unjust or biased decision. Regulatory examiners, often working in regulated industries, expect similar explanations. They are responsible for auditing ML models to ensure compliance with industry standards and ethical norms \cite{Chen2023Globally-ConsistentEvaluation, Flores2016FalseBlacks}. 

Finally, data analysts are increasingly utilizing ML models for tasks aimed at understanding data-generating processes in both industrial and scientific research contexts \cite{Freiesleben2022ScientificPhenomena, SilvaFilho2023AAchievement}. These models often supplant traditional statistical methods due to their flexibility in handling large volumes of data without requiring prior domain knowledge. Although ML models can offer high predictive accuracy, they may lack explanatory power, thereby impeding a comprehensive understanding of the phenomena under investigation. Incorporating interpretability can address this limitation by elucidating the relationships among variables, consequently facilitating hypothesis generation for subsequent research.

\subsection{Explanations scope}

In addition to the types of model explanation techniques and the nature of the data, explainers can be categorized based on their scope. Local explainers refer to individual predictions, while global explainers quantify the average behavior of a model. Specifically, explanations can be further categorized into feature effects, which are commonly expressed through graphs, and feature importance, which provides a score-based summary measure of the overall contribution of features.

Feature effects describe how the impact of a feature varies across its value range, using a simplified function $\hat{f}_S: Xs \xrightarrow{} Y$, being $Xs$ a set of features to be explained with a size typically of 1 or 2 features. Examples of global feature effects are Marginal Effects (ME) \cite{Long2021UsingOutcomes, Mize2019AModels}, ALE plots \cite{Apley2020VisualizingModels}, PD plots \cite{Friedman2001GreedyMachine.} and SHAP \cite{10.5555/3295222.3295230}. 

Score-based explanations, often referred to as feature importance, essentially provide a ranking of features based on how much each one decreases the model's prediction error. The most commonly used methods are the tree-based MDI \cite{Breiman2001RandomForests} and the model-agnostic PFI \cite{Fisher2018AllSimultaneously} and its variations \cite{Molnar2023Model-agnosticApproach, Strobl2008ConditionalForests}. Additionally, there are score-based versions of some feature effects techniques such as PD\cite{Greenwell2018AMeasure}, ME\cite{long1997regression} and SHAP\cite{Lee2023SHAPForecasting}.

\subsection{Can XAI really obtain knowledge about the world?}
\label{xai_can_fail_but_is_usable}

Discussing the cability of XAI to extract knowledge from the world is essential, given that the core argument of this thesis hinges on XAI being an invaluable tool for deriving trustworthy insights. This view is in line with the growing trend among researchers towards more transparent AI and ML models, as a response to their increasing integration in society \cite{Arya2019OneTechniques}. These researchers advocate that the empirical use of ML could pivot scientific research towards a theory-independent method, allowing data to convey its own story without pre-existing hypotheses about the data, as noted in various studies \cite{Kitchin2014BigShifts, Anderson2008TheObsolete, Naimi2014BigThink, Andrews2023TheIdeal, LiebersonImplicationSciences}.

While models known for their inherent transparency have faced minimal criticism, post-hoc techniques encounter more scrutiny  despite their widespread use across various fields such as education \cite{Lezhnina2022CombiningPISA, Martinez-Abad2020EducationalAssessment}, healthcare \cite{Jauhiainen2021NewAthletes, Stiglic2020InterpretabilityHealthcare}, social science \cite{Berger2023ExplainableChains, Bellantuono2023DetectingIntelligence}, and sociology \cite{Li2023ApplyingPandemic, Fan2023InterpretableInequality}.


The primary critique stems from the potential mismatch between what the opaque model is doing and what the post-hoc model attempts to explain \cite{Rudin2019StopInstead, Mullainathan2017MachineApproach, Babic2021BewareCare}. On the other hand, \cite{Sullivan2022UnderstandingModels} argues that gaining real-world knowledge with ML models is feasible as long as the link between model and phenomenon uncertainty can be assessed. Similarly, \cite{Cichy2019DeepModels} and \cite{Zednik2021SolvingIntelligence} suggest that XAI can aid in understanding the real world, but they remain vague about how the model and phenomenon are connected.

Through the lens of philosophy of science and epistemology, authors in \cite{Fleisher2022UnderstandingAI} draw parallels between XAI and the fundamental concepts of understanding. While there is some disagreement in the field, there is consensus that understanding is not an all-or-nothing state. Genuine understanding comes in degrees and can accommodate some inaccuracy and falsehood. In other words, understanding can still be valid even if the information or concepts it's based on are not entirely accurate. This ties into the concept of \textit{idealization} in scientific models, which refers to the process of simplifying or abstracting certain aspects of a phenomenon or model to make it more tractable \cite{Jebeile2015ExplainingIdealizations}. Building on this, \cite{Fleisher2022UnderstandingAI} argues that XAI research has a solid foundation in science and promising avenues.

Rudin and colleagues \cite{Rudin2019StopInstead} advocate for the use of inherently interpretable models rather than combining opaque models with post-hoc techniques, especially in high-stakes decisions. Although this may initially seem like a criticism of post-hoc techniques, the critique centers around the inappropriate selection of models that are too complex without much performance improvement. Nevertheless, the use of inherent interpretable models does not prohibit the application of post-hoc methods. In fact, post-hoc methods are model-agnostic and can be applied to any model and can provide extra insights. As \cite{Molnar2022Model-agnosticLearning} notes, inherent interpretable models should always be included in benchmarks. 

In this context, it is posited that XAI, particularly via post-hoc techniques, has the potential to augment model interpretability. Such enhancement is achieved either by providing additional insights into inherently interpretable models or by shedding light on functions of otherwise opaque models. And while these insights may not perfectly mirror the target model, they can be crucial in developing new theories based on real-world data. These theories can then be explored further to advance science and knowledge.
Therefore, results from ML explanations provide not an end goal, but the starting point for further analysis and conceptualization.


\input{chapters/2-backgrounds/definitions}






